
@article{mussmann_uncertainty_2018,
	title = {Uncertainty {Sampling} is {Preconditioned} {Stochastic} {Gradient} {Descent} on {Zero}-{One} {Loss}},
	url = {http://arxiv.org/abs/1812.01815},
	abstract = {Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it has been observed in practice to converge to different parameters depending on the initialization and sometimes to even better parameters than standard training on all the data. In this work, we give a theoretical explanation of this phenomenon, showing that uncertainty sampling on a convex loss can be interpreted as performing a preconditioned stochastic gradient step on a smoothed version of the population zero-one loss that converges to the population zero-one loss. Furthermore, uncertainty sampling moves in a descent direction and converges to stationary points of the smoothed population zero-one loss. Experiments on synthetic and real datasets support this connection.},
	urldate = {2020-05-26},
	journal = {arXiv:1812.01815 [cs, stat]},
	author = {Mussmann, Stephen and Liang, Percy},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01815},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/gianma/Zotero/storage/6JSWUGJC/1812.html:text/html;arXiv Fulltext PDF:/home/gianma/Zotero/storage/PYNBZNST/Mussmann and Liang - 2018 - Uncertainty Sampling is Preconditioned Stochastic .pdf:application/pdf}
}

@article{kottke_toward_2020,
	title = {Toward {Optimal} {Probabilistic} {Active} {Learning} {Using} a {Bayesian} {Approach}},
	url = {http://arxiv.org/abs/2006.01732},
	abstract = {Gathering labeled data to train well-performing machine learning models is one of the critical challenges in many applications. Active learning aims at reducing the labeling costs by an efﬁcient and effective allocation of costly labeling resources. In this article, we propose a decision-theoretic selection strategy that (1) directly optimizes the gain in misclassiﬁcation error, and (2) uses a Bayesian approach by introducing a conjugate prior distribution to determine the class posterior to deal with uncertainties. By reformulating existing selection strategies within our proposed model, we can explain which aspects are not covered in current state-of-the-art and why this leads to the superior performance of our approach. Extensive experiments on a large variety of datasets and different kernels validate our claims.},
	language = {en},
	urldate = {2020-08-19},
	journal = {arXiv:2006.01732 [cs, stat]},
	author = {Kottke, Daniel and Herde, Marek and Sandrock, Christoph and Huseljic, Denis and Krempl, Georg and Sick, Bernhard},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.01732},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, 68T05},
	file = {Kottke et al. - 2020 - Toward Optimal Probabilistic Active Learning Using.pdf:/home/gianma/Zotero/storage/L45WTMGX/Kottke et al. - 2020 - Toward Optimal Probabilistic Active Learning Using.pdf:application/pdf}
}

@incollection{kirsch_batchbald_2019,
	title = {{BatchBALD}: {Efficient} and {Diverse} {Batch} {Acquisition} for {Deep} {Bayesian} {Active} {Learning}},
	shorttitle = {{BatchBALD}},
	url = {http://papers.nips.cc/paper/8925-batchbald-efficient-and-diverse-batch-acquisition-for-deep-bayesian-active-learning.pdf},
	urldate = {2020-09-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {7026--7037},
	file = {NIPS Full Text PDF:/home/gianma/Zotero/storage/6VCBCZL9/Kirsch et al. - 2019 - BatchBALD Efficient and Diverse Batch Acquisition.pdf:application/pdf;NIPS Snapshot:/home/gianma/Zotero/storage/BLEJLGRM/8925-batchbald-efficient-and-diverse-batch-acquisition-for-deep-bayesian-active-learning.html:text/html}
}

@article{imberg_optimal_nodate,
	title = {Optimal sampling in unbiased active learning},
	abstract = {A common belief in unbiased active learning is that, in order to capture the most informative instances, the sampling probabilities should be proportional to the uncertainty of the class labels. We argue that this produces suboptimal predictions and present sampling schemes for unbiased pool-based active learning that minimise the actual prediction error, and demonstrate a better predictive performance than competing methods on a number of benchmark datasets. In contrast, both probabilistic and deterministic uncertainty sampling performed worse than simple random sampling on some of the datasets.},
	language = {en},
	author = {Imberg, Henrik and Jonasson, Johan and Axelson-Fisk, Marina},
	pages = {10},
	file = {Imberg et al. - Optimal sampling in unbiased active learning.pdf:/home/gianma/Zotero/storage/GLMKBSRF/Imberg et al. - Optimal sampling in unbiased active learning.pdf:application/pdf}
}
